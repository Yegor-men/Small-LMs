{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import statements",
   "id": "7fac61a9935602e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:48:11.858871Z",
     "start_time": "2025-04-11T02:48:10.021212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast"
   ],
   "id": "b5acac93baacec66",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data loading",
   "id": "264382bafc76e676"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-11T02:48:11.883564Z",
     "start_time": "2025-04-11T02:48:11.867191Z"
    }
   },
   "source": [
    "import duckdb\n",
    "from typing import Iterator, List, Optional\n",
    "\n",
    "def get_training_corpus(\n",
    "        db_path: str = \"../../data/fineweb/fineweb.db\",\n",
    "        batch_size: int = 1_000,\n",
    "        max_entries: Optional[int] = None\n",
    ") -> Iterator[List[str]]:\n",
    "    \"\"\"\n",
    "    Stream batches of 'text' out of the fineweb DuckDB table.\n",
    "\n",
    "    Args:\n",
    "        db_path:      Path to your .db file.\n",
    "        batch_size:   How many rows to pull per batch.\n",
    "        max_entries:  If set, stops after yielding this many total rows.\n",
    "\n",
    "    Yields:\n",
    "        Lists of `batch_size` text strings (last batch may be smaller).\n",
    "    \"\"\"\n",
    "    conn = duckdb.connect(db_path)\n",
    "    # Open a cursor on the text column only\n",
    "    cur = conn.cursor().execute(\"SELECT text FROM fineweb\")\n",
    "    total_yielded = 0\n",
    "\n",
    "    while True:\n",
    "        # fetchmany is efficient and avoids OFFSET\n",
    "        rows = cur.fetchmany(batch_size)\n",
    "        if not rows:\n",
    "            break\n",
    "\n",
    "        texts = [r[0] for r in rows]\n",
    "\n",
    "        # If the caller only wants the first N entries, truncate and stop\n",
    "        if max_entries is not None:\n",
    "            remaining = max_entries - total_yielded\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "            if len(texts) > remaining:\n",
    "                texts = texts[:remaining]\n",
    "                yield texts\n",
    "                break\n",
    "\n",
    "        yield texts\n",
    "        total_yielded += len(texts)\n",
    "\n",
    "    conn.close()\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialization/training",
   "id": "5c68eb4070e02039"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:48:20.557708Z",
     "start_time": "2025-04-11T02:48:11.918522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "special_tokens = [\"<eos>\", \"<pad>\", \"<unk>\", \"<model>\", \"</model>\", \"<user>\", \"</user>\", \"<system>\", \"</system>\", \"<think>\", \"</think>\",]\n",
    "total = len(special_tokens) + 50_000\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=16384,\n",
    "    special_tokens=special_tokens,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    ")\n",
    "tokenizer.train_from_iterator(get_training_corpus(max_entries = 100000), trainer=trainer)\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()"
   ],
   "id": "c6403cb857fa2909",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing",
   "id": "c15547d4842797f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:48:20.574340Z",
     "start_time": "2025-04-11T02:48:20.567030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.ids)\n",
    "print(tokenizer.decode(encoding.ids))\n",
    "print(len(tokenizer.get_vocab()))\n",
    "print(tokenizer.get_vocab_size())"
   ],
   "id": "a37327cf8395b8ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', \"'s\", 'Ġtest', 'Ġthis', 'Ġto', 'ken', 'izer', '.']\n",
      "[11587, 416, 1481, 433, 294, 4196, 7392, 24]\n",
      "Let's test this tokenizer.\n",
      "16384\n",
      "16384\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Saving",
   "id": "fdc2d8d7e9e6bf6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:48:20.642132Z",
     "start_time": "2025-04-11T02:48:20.610968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    pad_token=\"<pad>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    # cls_token=\"[CLS]\",\n",
    "    # sep_token=\"[SEP]\",\n",
    "    # mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "wrapped_tokenizer.save_pretrained(\"../../../saved_models/tokenizers/nanogpt/nanogpt\")"
   ],
   "id": "1cf126076556d1f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../../saved_models/tokenizers/nanogpt/nanogpt/tokenizer_config.json',\n",
       " '../../../saved_models/tokenizers/nanogpt/nanogpt/special_tokens_map.json',\n",
       " '../../../saved_models/tokenizers/nanogpt/nanogpt/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T02:48:20.706034Z",
     "start_time": "2025-04-11T02:48:20.681409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tok = AutoTokenizer.from_pretrained(\"../../../saved_models/tokenizers/nanogpt/nanogpt\")\n",
    "\n",
    "tokens = tok.tokenize(\"Test number 2.!@#$%^&*() Yohoho skibidi biden <think></think><eos>\")\n",
    "\n",
    "print(tokens)\n",
    "ids = tok.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "decoded_string = tok.decode(ids)\n",
    "print(decoded_string)\n",
    "print(len(tok.get_vocab()))"
   ],
   "id": "c3bd6341db5e0347",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'est', 'Ġnumber', 'Ġ2', '.', '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', 'ĠY', 'oh', 'oh', 'o', 'Ġsk', 'ib', 'id', 'i', 'Ġbid', 'en', 'Ġ', '<think>', '</think>', '<eos>']\n",
      "[62, 404, 1366, 388, 24, 11, 42, 13, 14, 15, 72, 16, 20, 18, 19, 554, 1111, 1111, 89, 1232, 582, 330, 83, 8642, 284, 231, 9, 10, 0]\n",
      "Test number 2.!@#$%^&*() Yohoho skibidi biden <think></think><eos>\n",
      "16384\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
