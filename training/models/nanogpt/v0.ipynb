{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import statements",
   "id": "305ec5c171e708c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import inspect\n",
    "import logging\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.amp import autocast, GradScaler\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "device = \"cuda\""
   ],
   "id": "cb074eb36e19a92b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Decoder block architecture",
   "id": "70bafaab87f7e36b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "class CasualMaskedDecoderBlocks(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim: int,\n",
    "            num_heads: int,\n",
    "            num_blocks: int,\n",
    "            max_seq_len: int,\n",
    "            dropout: float = 0.0,\n",
    "            activation_function: str = \"gelu\",\n",
    "            ffw_network_multiplier: int = 4,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param embed_dim: Embedding dimension of the tokens in the sequence.\n",
    "        :param num_heads: Number of heads in each decoder block.\n",
    "        :param num_blocks: Number of decoder blocks.\n",
    "        :param dropout: Probability of dropout.\n",
    "        :param max_seq_len: Maximum expected sequence length.\n",
    "        :param ffw_network_multiplier: multiplier for embed_dim to get the dimensionality for the feedforward network.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: embed_dim ({embed_dim}) is not divisible by num_heads ({num_heads})'\n",
    "\n",
    "        assert activation_function in (\"relu\",\n",
    "                                       \"gelu\"), f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: activation_function expected to be \"relu\"/\"gelu\", received \"{activation_function}\" instead'\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        block = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ffw_network_multiplier * embed_dim,\n",
    "            dropout=dropout,\n",
    "            activation=activation_function,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.ModuleList([block for _ in range(num_blocks)])\n",
    "\n",
    "        causal = torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1).bool()\n",
    "        self.register_buffer(\"causal_mask\", causal)\n",
    "\n",
    "    def forward(self, tok_seq, padding_mask=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param tok_seq: torch.Tensor of size [batch, seq_len, embed_dim] representing the pre-attended token sequence.\n",
    "        :param padding_mask: Bool mask of size [batch, seq_len]; True for padding tokens.\n",
    "        :return: Attended token sequence after num_blocks amount of decoder blocks.\n",
    "        \"\"\"\n",
    "        assert tok_seq.dim() == 3, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: tok_seq tensor should be of size [batch, seq_len, embed_dim], received a tensor with {tok_seq.dim()} dimensions instead'\n",
    "        batch_size, seq_len, embed_dim = tok_seq.size()\n",
    "        assert seq_len <= self.max_seq_len, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: length of inputted sequence ({seq_len}) exceeds maximum expected sequence length ({self.max_seq_len})'\n",
    "        assert embed_dim == self.embed_dim, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: received embed_dim ({embed_dim}) does not match the expected embed_dim ({self.embed_dim})'\n",
    "        if padding_mask is not None:\n",
    "            assert padding_mask.dim() == 2, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: padding_mask tensor should be of size [batch, seq_len], received a tensor with {padding_mask.dim()} dimensions instead'\n",
    "            pm_batch_size, pm_seq_len = padding_mask.size()\n",
    "            assert batch_size == pm_batch_size and seq_len == pm_seq_len, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: dimension mismatch between tok_seq ([{batch_size},{seq_len},{embed_dim}]) and padding_mask ([{pm_batch_size}, {pm_seq_len}])'\n",
    "\n",
    "        casual_mask = self.causal_mask[:seq_len, :seq_len]\n",
    "\n",
    "        for block in self.blocks:\n",
    "            tok_seq = block(\n",
    "                src=tok_seq,\n",
    "                src_mask=casual_mask,\n",
    "                src_key_padding_mask=padding_mask\n",
    "            )\n",
    "\n",
    "        return tok_seq"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Attended token decoder",
   "id": "13b6365ba34ec91c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class AttendedTokenDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim: int,\n",
    "            vocabulary_size: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param embed_dim:\n",
    "        :param vocabulary_size:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=embed_dim, out_features=vocabulary_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, att_tok_seq):\n",
    "        \"\"\"\n",
    "        :param att_tok_seq:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert att_tok_seq.dim() == 3, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: att_tok_seq tensor should be of size [batch, seq_len, embed_dim], received a tensor with {att_tok_seq.dim()} dimensions instead'\n",
    "\n",
    "        batch_size, seq_len, embed_dim = att_tok_seq.size()\n",
    "\n",
    "        assert embed_dim == self.embed_dim, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: received embed_dim ({embed_dim}) does not match expected embed_dim ({self.embed_dim})'\n",
    "\n",
    "        token_logits = self.decoder(att_tok_seq)\n",
    "\n",
    "        return token_logits"
   ],
   "id": "528f988946092b48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Combined GPT model",
   "id": "d45636d1aba86217"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim: int,\n",
    "            num_heads: int,\n",
    "            num_blocks: int,\n",
    "            max_seq_len: int,\n",
    "            vocab_size: int,\n",
    "            tokenizer,\n",
    "            dropout: float = 0.0,\n",
    "            activation_function: str = \"gelu\",\n",
    "            ffw_network_multiplier: int = 4,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param embed_dim:\n",
    "        :param num_heads:\n",
    "        :param num_blocks:\n",
    "        :param max_seq_len:\n",
    "        :param vocab_size:\n",
    "        :param tokenizer:\n",
    "        :param dropout:\n",
    "        :param activation_function:\n",
    "        :param ffw_network_multiplier:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.activation_function = activation_function\n",
    "        self.ffw_network_multiplier = ffw_network_multiplier\n",
    "\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "\n",
    "        self.decoder_blocks = CasualMaskedDecoderBlocks(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            num_blocks=num_blocks,\n",
    "            max_seq_len=max_seq_len,\n",
    "            dropout=dropout,\n",
    "            activation_function=activation_function,\n",
    "            ffw_network_multiplier=ffw_network_multiplier,\n",
    "        )\n",
    "\n",
    "        self.decoder = AttendedTokenDecoder(\n",
    "            embed_dim=embed_dim,\n",
    "            vocabulary_size=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, tokenized_sequence):\n",
    "        \"\"\"\n",
    "        :param tokenized_sequence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        padding_mask = tokenized_sequence.eq(self.pad_token_id)\n",
    "\n",
    "        batch_size, seq_length = tokenized_sequence.size()\n",
    "\n",
    "        embedded_sequence = self.token_embedding(tokenized_sequence)\n",
    "\n",
    "        position_ids = (\n",
    "            torch.arange(seq_length, device=embedded_sequence.device)\n",
    "            .unsqueeze(0)\n",
    "            .expand(batch_size, seq_length)\n",
    "        )\n",
    "        positional_embeddings = self.positional_embedding(position_ids)\n",
    "\n",
    "        sequence = embedded_sequence + positional_embeddings\n",
    "\n",
    "        sequence = self.decoder_blocks(sequence, padding_mask)\n",
    "\n",
    "        logits = self.decoder(sequence)\n",
    "\n",
    "        return logits"
   ],
   "id": "f9e3b00b1fa46b44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialization",
   "id": "a7400b765d05aa11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer_model_path = \"../../../saved_models/tokenizers/nanogpt/nanogpt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_path)\n",
    "\n",
    "EMBED_DIM = 512\n",
    "NUM_HEADS = 8\n",
    "NUM_BLOCKS = 18\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "VOCAB_SIZE = len(tokenizer.get_vocab())\n",
    "\n",
    "model = GPTModel(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_blocks=NUM_BLOCKS,\n",
    "    max_seq_len=MAX_SEQ_LENGTH,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    tokenizer=tokenizer,\n",
    "    dropout=0.0,\n",
    ").to(device)"
   ],
   "id": "c771703b89e8d2b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total: {total_params:,}\")\n",
    "print(f\"Trainable: {total_params:,}\")"
   ],
   "id": "3b406efb57c2dc3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data loading",
   "id": "8c8002fa352c9779"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import duckdb\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "class ChunkedFineWebDataset(IterableDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            db_path: str,\n",
    "            tokenizer: PreTrainedTokenizerFast,\n",
    "            split: str = \"train\",\n",
    "            val_mod: int = 0,\n",
    "            num_buckets: int = 10,\n",
    "            samples_per_epoch: int = 10,\n",
    "            batch_size: int = 16,\n",
    "            max_length: int = 512,\n",
    "            stride: int = 256,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Yields lists of token IDs of length <= max_length+1.\n",
    "        \"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.split = split\n",
    "        self.val_mod = val_mod\n",
    "        self.num_buckets = num_buckets\n",
    "        self.samples_per_epoch = samples_per_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "\n",
    "    def __iter__(self):\n",
    "        conn = duckdb.connect(self.db_path)\n",
    "        if self.split == \"train\":\n",
    "            where = f\"abs(hash(id)) % {self.num_buckets} != {self.val_mod}\"\n",
    "        else:\n",
    "            where = f\"abs(hash(id)) % {self.num_buckets} = {self.val_mod}\"\n",
    "\n",
    "        batches = self.samples_per_epoch // self.batch_size\n",
    "        for _ in range(batches):\n",
    "            # reservoir‐sample exactly batch_size rows\n",
    "            query = f\"\"\"\n",
    "            SELECT text\n",
    "            FROM fineweb\n",
    "            TABLESAMPLE RESERVOIR({self.batch_size})\n",
    "            WHERE {where}\n",
    "            \"\"\"\n",
    "            rows = conn.execute(query).fetchall()\n",
    "\n",
    "            # fallback if reservoir returns fewer rows (rare)\n",
    "            if len(rows) < self.batch_size:\n",
    "                rows = conn.execute(f\"\"\"\n",
    "                  SELECT text\n",
    "                  FROM fineweb\n",
    "                  WHERE {where}\n",
    "                  LIMIT {self.batch_size}\n",
    "                \"\"\").fetchall()\n",
    "\n",
    "            for (txt,) in rows:\n",
    "                txt = txt.replace(\"\\n\", \" \").strip()\n",
    "                token_ids = self.tokenizer.encode(txt)\n",
    "                start = 0\n",
    "                while start < len(token_ids):\n",
    "                    window = token_ids[start: start + self.max_length + 1]\n",
    "                    if len(window) < 2:\n",
    "                        break\n",
    "                    yield window\n",
    "                    start += (self.max_length - self.stride)\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        # #windows is approximate: (samples_per_epoch * avg_tokens) / (max_length-stride)\n",
    "        return (self.samples_per_epoch // self.batch_size) * self.batch_size\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class ChunkedCollator:\n",
    "    def __init__(self, pad_token_id: int):\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # batch: List[List[int]] of length BATCH_SIZE\n",
    "        input_ids = [torch.tensor(ids[:-1], dtype=torch.long) for ids in batch]\n",
    "        labels = [torch.tensor(ids[1:], dtype=torch.long) for ids in batch]\n",
    "\n",
    "        # pad to longest in batch\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.pad_token_id)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "SAMPLES_PE = 160\n",
    "\n",
    "# 2) Datasets\n",
    "train_ds = ChunkedFineWebDataset(\n",
    "    db_path=\"../../data/fineweb/fineweb.db\",\n",
    "    tokenizer=tokenizer,\n",
    "    split=\"train\",\n",
    "    samples_per_epoch=SAMPLES_PE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    stride=256\n",
    ")\n",
    "val_ds = ChunkedFineWebDataset(\n",
    "    db_path=\"../../data/fineweb/fineweb.db\",\n",
    "    tokenizer=tokenizer,\n",
    "    split=\"val\",\n",
    "    samples_per_epoch=SAMPLES_PE,  # smaller val\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    stride=256\n",
    ")\n",
    "\n",
    "# 3) Collator\n",
    "collator = ChunkedCollator(pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "# 4) DataLoaders\n",
    "train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=collator)\n",
    "valid_dataloader = DataLoader(val_ds, batch_size=BATCH_SIZE, collate_fn=collator)"
   ],
   "id": "668b8cfe793537be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## How to save stuff",
   "id": "16d4e96e9e74db40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "\n",
    "def save_checkpoint(step, avg_loss, avg_entropy):\n",
    "    ckpt = {\n",
    "        \"step\": step,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"scaler_state_dict\": scaler.state_dict(),\n",
    "        \"loss\": avg_loss,\n",
    "        \"entropy\": avg_entropy,\n",
    "    }\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    fname = f\"../../../saved_models/models/nanogpt/nanogpt-S{step+1:05d}-L{avg_loss:.4f}-E{avg_entropy:.4f}-{ts}.pt\"\n",
    "    torch.save(ckpt, fname)\n",
    "    print(f\"Saved checkpoint to {fname}\")\n"
   ],
   "id": "4765ab14a2706f2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training loop",
   "id": "d747ad7bf7cc9b83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "NUM_EPOCHS = 8_500\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "scaler = GradScaler()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "num_warmup_steps = int(0.1 * NUM_EPOCHS)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=NUM_EPOCHS\n",
    ")\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "learning_rates = []\n",
    "train_entropies = []\n",
    "valid_entropies = []\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "\"\"\"This stuff for resuming training if I stop it\"\"\"\n",
    "\n",
    "# ckpt = torch.load(\"../../../saved_models/models/nanogpt/S01999-L9.1571-E7.6775-20250410_2232.pt\")\n",
    "# model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "# optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "# scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
    "# scaler.load_state_dict(ckpt[\"scaler_state_dict\"])\n",
    "# start_step = ckpt[\"step\"]\n",
    "\n",
    "# for epoch in range(start_step, NUM_EPOCHS):\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    to_print = True if ((epoch + 1) % 1 == 0 or epoch == 0) else False\n",
    "    print(f\"Epoch {(epoch + 1)}/{NUM_EPOCHS} - {((epoch + 1) / NUM_EPOCHS) * 100:.3f}%\",\n",
    "          end=\" | \") if to_print else None\n",
    "\n",
    "    train_loss = 0\n",
    "    train_entropy = 0\n",
    "    skipped = 0\n",
    "\n",
    "    for index, batch in enumerate(train_dataloader):\n",
    "        last_lr = scheduler.get_last_lr()[0]\n",
    "        learning_rates.append(last_lr)\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        if input_ids.size(0) != BATCH_SIZE:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with autocast(\"cuda\"):\n",
    "            logits = model(input_ids)\n",
    "            probability_dist = F.softmax(logits[:, -1, :], dim=-1)\n",
    "            entropy = torch.mean(Categorical(probs=probability_dist).entropy())\n",
    "            train_entropy += entropy.detach().item()\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, len(tokenizer.get_vocab())),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    effective_train_dataloader_length = len(train_dataloader) - skipped\n",
    "    scaler.unscale_(optimizer)\n",
    "    # clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_loss /= effective_train_dataloader_length\n",
    "    train_losses.append(train_loss)\n",
    "    train_entropy /= effective_train_dataloader_length\n",
    "    train_entropies.append(train_entropy)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.5f} E {train_entropy:5f}\", end=\" | \") if to_print else None\n",
    "    print(f\"LR: {scheduler.get_last_lr()[0]:7f}\", end=\" | \") if to_print else None\n",
    "\n",
    "    valid_loss = 0\n",
    "    valid_entropy = 0\n",
    "    skipped = 0\n",
    "\n",
    "    for index, batch in enumerate(valid_dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        if input_ids.size(0) != BATCH_SIZE:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with autocast(\"cuda\"):\n",
    "            logits = model(input_ids)\n",
    "            probability_dist = F.softmax(logits[:, -1, :], dim=-1)\n",
    "            entropy = torch.mean(Categorical(probs=probability_dist).entropy())\n",
    "            valid_entropy += entropy.detach().item()\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, len(tokenizer.get_vocab())),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "    effective_valid_dataloader_length = len(valid_dataloader) - skipped\n",
    "    valid_loss /= effective_valid_dataloader_length\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_entropy /= effective_valid_dataloader_length\n",
    "    valid_entropies.append(valid_entropy)\n",
    "\n",
    "    print(f\"Valid Loss: {valid_loss:.5f} E {valid_entropy:5f}\") if to_print else None\n",
    "\n",
    "    if ((epoch + 1) % 100 == 0) or ((epoch + 1) == NUM_EPOCHS):\n",
    "        save_checkpoint(epoch, valid_loss, valid_entropy)\n",
    "\n",
    "print(\"finito\")"
   ],
   "id": "cf85d042c558871a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes[0].plot(train_losses, label=\"Train losses\")\n",
    "axes[0].plot(valid_losses, label=\"Validation losses\")\n",
    "axes[0].set_title(\"Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(train_entropies, label=\"Train entropies\")\n",
    "axes[1].plot(valid_entropies, label=\"Validation entropies\")\n",
    "axes[1].set_title(\"Entropy\")\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(learning_rates, label=\"LR values\")\n",
    "axes[2].set_title(\"Learning rate values over time\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "id": "5f85e3c1ea1e288f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
