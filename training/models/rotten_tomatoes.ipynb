{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import statements",
   "id": "305ec5c171e708c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T11:56:35.868959Z",
     "start_time": "2025-04-07T11:56:35.867192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import inspect\n",
    "import logging\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import pathlib\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    ")"
   ],
   "id": "cb074eb36e19a92b",
   "outputs": [],
   "execution_count": 186
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Decoder block architecture",
   "id": "70bafaab87f7e36b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-07T11:56:35.884033Z",
     "start_time": "2025-04-07T11:56:35.880577Z"
    }
   },
   "source": [
    "class CasualMaskedDecoderBlocks(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim: int,\n",
    "            num_heads: int,\n",
    "            num_blocks: int,\n",
    "            max_seq_len: int,\n",
    "            dropout: float = 0.0,\n",
    "            activation_function: str = \"gelu\",\n",
    "            ffw_network_multiplier: int = 4,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param embed_dim: Embedding dimension of the tokens in the sequence.\n",
    "        :param num_heads: Number of heads in each decoder block.\n",
    "        :param num_blocks: Number of decoder blocks.\n",
    "        :param dropout: Probability of dropout.\n",
    "        :param max_seq_len: Maximum expected sequence length.\n",
    "        :param ffw_network_multiplier: multiplier for embed_dim to get the dimensionality for the feedforward network.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: embed_dim ({embed_dim}) is not divisible by num_heads ({num_heads})'\n",
    "\n",
    "        assert activation_function in (\"relu\",\n",
    "                                       \"gelu\"), f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: activation_function expected to be \"relu\"/\"gelu\", received \"{activation_function}\" instead'\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        block = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ffw_network_multiplier * embed_dim,\n",
    "            dropout=dropout,\n",
    "            activation=activation_function,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.ModuleList([block for _ in range(num_blocks)])\n",
    "\n",
    "        causal = torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1).bool()\n",
    "        self.register_buffer(\"causal_mask\", causal)\n",
    "\n",
    "    def forward(self, tok_seq, padding_mask=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param tok_seq: torch.Tensor of size [batch, seq_len, embed_dim] representing the pre-attended token sequence.\n",
    "        :param padding_mask: Bool mask of size [batch, seq_len]; True for padding tokens.\n",
    "        :return: Attended token sequence after num_blocks amount of decoder blocks.\n",
    "        \"\"\"\n",
    "        assert tok_seq.dim() == 3, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: tok_seq tensor should be of size [batch, seq_len, embed_dim], received a tensor with {tok_seq.dim()} dimensions instead'\n",
    "        batch_size, seq_len, embed_dim = tok_seq.size()\n",
    "        assert seq_len <= self.max_seq_len, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: length of inputted sequence ({seq_len}) exceeds maximum expected sequence length ({self.max_seq_len})'\n",
    "        assert embed_dim == self.embed_dim, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: received embed_dim ({embed_dim}) does not match the expected embed_dim ({self.embed_dim})'\n",
    "        if padding_mask is not None:\n",
    "            assert padding_mask.dim() == 2, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: padding_mask tensor should be of size [batch, seq_len], received a tensor with {padding_mask.dim()} dimensions instead'\n",
    "            pm_batch_size, pm_seq_len = padding_mask.size()\n",
    "            assert batch_size == pm_batch_size and seq_len == pm_seq_len, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: dimension mismatch between tok_seq ([{batch_size},{seq_len},{embed_dim}]) and padding_mask ([{pm_batch_size}, {pm_seq_len}])'\n",
    "\n",
    "        casual_mask = self.causal_mask[:seq_len, :seq_len]\n",
    "\n",
    "        for block in self.blocks:\n",
    "            tok_seq = block(\n",
    "                src=tok_seq,\n",
    "                src_mask=casual_mask,\n",
    "                src_key_padding_mask=padding_mask\n",
    "            )\n",
    "\n",
    "        return tok_seq"
   ],
   "outputs": [],
   "execution_count": 187
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Attended token decoder",
   "id": "13b6365ba34ec91c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T11:56:35.923338Z",
     "start_time": "2025-04-07T11:56:35.921366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AttendedTokenDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim, vocabulary_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=embed_dim, out_features=vocabulary_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, att_tok_seq):\n",
    "        assert att_tok_seq.dim() == 3, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: att_tok_seq tensor should be of size [batch, seq_len, embed_dim], received a tensor with {att_tok_seq.dim()} dimensions instead'\n",
    "\n",
    "        batch_size, seq_len, embed_dim = att_tok_seq.size()\n",
    "\n",
    "        assert embed_dim == self.embed_dim, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: received embed_dim ({embed_dim}) does not match expected embed_dim ({self.embed_dim})'\n",
    "\n",
    "        token_logits = self.decoder(att_tok_seq)\n",
    "\n",
    "        return token_logits"
   ],
   "id": "528f988946092b48",
   "outputs": [],
   "execution_count": 188
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Combined GPT model",
   "id": "d45636d1aba86217"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T11:56:36.011741Z",
     "start_time": "2025-04-07T11:56:36.008946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim: int,\n",
    "            num_heads: int,\n",
    "            num_blocks: int,\n",
    "            max_seq_len: int,\n",
    "            tokenizer: str | PreTrainedTokenizerFast,\n",
    "            dropout: float = 0.0,\n",
    "            activation_function: str = \"gelu\",\n",
    "            ffw_network_multiplier: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(tokenizer, (str, pathlib.Path)):\n",
    "            self.tokenizer = PreTrainedTokenizerFast.from_pretrained(str(tokenizer))\n",
    "        elif isinstance(tokenizer, PreTrainedTokenizerFast):\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            raise ValueError(f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: tokenizer must be a path or PreTrainedTokenizerFast, got {type(tokenizer)} instead')\n",
    "\n",
    "        self.vocabulary_size = self.tokenizer.vocab_size\n",
    "        self.pad_id = self.tokenizer.pad_token_id\n",
    "\n",
    "        if embed_dim < self.vocabulary_size: logging.warning(\n",
    "            f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: embed_dim is smaller than vocabulary_size, consider increasing embed_dim')\n",
    "\n",
    "        self.token_embedding = nn.Embedding(self.vocabulary_size, embed_dim)\n",
    "        self.positional_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "\n",
    "        self.decoder_blocks = CasualMaskedDecoderBlocks(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            num_blocks=num_blocks,\n",
    "            max_seq_len=max_seq_len,\n",
    "            dropout=dropout,\n",
    "            activation_function=activation_function,\n",
    "            ffw_network_multiplier=ffw_network_multiplier,\n",
    "        )\n",
    "\n",
    "        self.decoder = AttendedTokenDecoder(\n",
    "            embed_dim=embed_dim,\n",
    "            vocabulary_size=self.vocabulary_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, tokenized_sequence):\n",
    "        pad_id = self.tokenizer.pad_token_id\n",
    "        padding_mask = tokenized_sequence.eq(pad_id)\n",
    "\n",
    "        batch_size, seq_length = tokenized_sequence.size()\n",
    "\n",
    "        embedded_sequence = self.embedding(tokenized_sequence)\n",
    "\n",
    "        position_ids = (\n",
    "            torch.arange(seq_length, device=embedded_sequence.device)\n",
    "            .unsqueeze(0)\n",
    "            .expand(batch_size, seq_length)\n",
    "        )\n",
    "        positional_embeddings = self.positional_embedding(position_ids)\n",
    "\n",
    "        sequence = embedded_sequence + positional_embeddings\n",
    "\n",
    "        sequence = self.transformer_blocks(sequence, padding_mask)\n",
    "\n",
    "        logits = self.decoder(sequence)\n",
    "\n",
    "        return logits"
   ],
   "id": "f9e3b00b1fa46b44",
   "outputs": [],
   "execution_count": 190
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialization",
   "id": "a7400b765d05aa11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T11:56:36.111377Z",
     "start_time": "2025-04-07T11:56:36.053747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test = torch.randn(1, 3, 1000)\n",
    "\n",
    "model = GPTModel(\n",
    "    embed_dim=1000,\n",
    "    num_heads=10,\n",
    "    num_blocks=10,\n",
    "    max_seq_len= 1024,\n",
    "    tokenizer= \"ski/buddy/rizz\",\n",
    ")"
   ],
   "id": "c771703b89e8d2b7",
   "outputs": [],
   "execution_count": 191
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
