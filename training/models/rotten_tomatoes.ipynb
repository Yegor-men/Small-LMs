{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import statements",
   "id": "305ec5c171e708c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import inspect\n",
    "import logging\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from torch.amp import autocast, GradScaler\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "device = \"cuda\""
   ],
   "id": "cb074eb36e19a92b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Decoder block architecture",
   "id": "70bafaab87f7e36b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "class CasualMaskedDecoderBlocks(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim: int,\n",
    "            num_heads: int,\n",
    "            num_blocks: int,\n",
    "            max_seq_len: int,\n",
    "            dropout: float = 0.0,\n",
    "            activation_function: str = \"gelu\",\n",
    "            ffw_network_multiplier: int = 4,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param embed_dim: Embedding dimension of the tokens in the sequence.\n",
    "        :param num_heads: Number of heads in each decoder block.\n",
    "        :param num_blocks: Number of decoder blocks.\n",
    "        :param dropout: Probability of dropout.\n",
    "        :param max_seq_len: Maximum expected sequence length.\n",
    "        :param ffw_network_multiplier: multiplier for embed_dim to get the dimensionality for the feedforward network.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: embed_dim ({embed_dim}) is not divisible by num_heads ({num_heads})'\n",
    "\n",
    "        assert activation_function in (\"relu\",\n",
    "                                       \"gelu\"), f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: activation_function expected to be \"relu\"/\"gelu\", received \"{activation_function}\" instead'\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        block = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ffw_network_multiplier * embed_dim,\n",
    "            dropout=dropout,\n",
    "            activation=activation_function,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.ModuleList([block for _ in range(num_blocks)])\n",
    "\n",
    "        causal = torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1).bool()\n",
    "        self.register_buffer(\"causal_mask\", causal)\n",
    "\n",
    "    def forward(self, tok_seq, padding_mask=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param tok_seq: torch.Tensor of size [batch, seq_len, embed_dim] representing the pre-attended token sequence.\n",
    "        :param padding_mask: Bool mask of size [batch, seq_len]; True for padding tokens.\n",
    "        :return: Attended token sequence after num_blocks amount of decoder blocks.\n",
    "        \"\"\"\n",
    "        assert tok_seq.dim() == 3, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: tok_seq tensor should be of size [batch, seq_len, embed_dim], received a tensor with {tok_seq.dim()} dimensions instead'\n",
    "        batch_size, seq_len, embed_dim = tok_seq.size()\n",
    "        assert seq_len <= self.max_seq_len, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: length of inputted sequence ({seq_len}) exceeds maximum expected sequence length ({self.max_seq_len})'\n",
    "        assert embed_dim == self.embed_dim, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: received embed_dim ({embed_dim}) does not match the expected embed_dim ({self.embed_dim})'\n",
    "        if padding_mask is not None:\n",
    "            assert padding_mask.dim() == 2, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: padding_mask tensor should be of size [batch, seq_len], received a tensor with {padding_mask.dim()} dimensions instead'\n",
    "            pm_batch_size, pm_seq_len = padding_mask.size()\n",
    "            assert batch_size == pm_batch_size and seq_len == pm_seq_len, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: dimension mismatch between tok_seq ([{batch_size},{seq_len},{embed_dim}]) and padding_mask ([{pm_batch_size}, {pm_seq_len}])'\n",
    "\n",
    "        casual_mask = self.causal_mask[:seq_len, :seq_len]\n",
    "\n",
    "        for block in self.blocks:\n",
    "            tok_seq = block(\n",
    "                src=tok_seq,\n",
    "                src_mask=casual_mask,\n",
    "                src_key_padding_mask=padding_mask\n",
    "            )\n",
    "\n",
    "        return tok_seq"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Attended token decoder",
   "id": "13b6365ba34ec91c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class AttendedTokenDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim: int,\n",
    "            vocabulary_size: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param embed_dim:\n",
    "        :param vocabulary_size:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=embed_dim, out_features=vocabulary_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, att_tok_seq):\n",
    "        \"\"\"\n",
    "        :param att_tok_seq:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert att_tok_seq.dim() == 3, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: att_tok_seq tensor should be of size [batch, seq_len, embed_dim], received a tensor with {att_tok_seq.dim()} dimensions instead'\n",
    "\n",
    "        batch_size, seq_len, embed_dim = att_tok_seq.size()\n",
    "\n",
    "        assert embed_dim == self.embed_dim, f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: received embed_dim ({embed_dim}) does not match expected embed_dim ({self.embed_dim})'\n",
    "\n",
    "        token_logits = self.decoder(att_tok_seq)\n",
    "\n",
    "        return token_logits"
   ],
   "id": "528f988946092b48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Combined GPT model",
   "id": "d45636d1aba86217"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim: int,\n",
    "            num_heads: int,\n",
    "            num_blocks: int,\n",
    "            max_seq_len: int,\n",
    "            vocab_size: int,\n",
    "            tokenizer,\n",
    "            dropout: float = 0.0,\n",
    "            activation_function: str = \"gelu\",\n",
    "            ffw_network_multiplier: int = 4,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param embed_dim:\n",
    "        :param num_heads:\n",
    "        :param num_blocks:\n",
    "        :param max_seq_len:\n",
    "        :param vocab_size:\n",
    "        :param tokenizer:\n",
    "        :param dropout:\n",
    "        :param activation_function:\n",
    "        :param ffw_network_multiplier:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.activation_function = activation_function\n",
    "        self.ffw_network_multiplier = ffw_network_multiplier\n",
    "\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        if embed_dim < self.vocab_size: logging.warning(\n",
    "            f'{self.__class__.__name__}.{inspect.currentframe().f_code.co_name}: embed_dim is smaller than vocabulary_size, consider increasing embed_dim')\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "\n",
    "        self.decoder_blocks = CasualMaskedDecoderBlocks(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            num_blocks=num_blocks,\n",
    "            max_seq_len=max_seq_len,\n",
    "            dropout=dropout,\n",
    "            activation_function=activation_function,\n",
    "            ffw_network_multiplier=ffw_network_multiplier,\n",
    "        )\n",
    "\n",
    "        self.decoder = AttendedTokenDecoder(\n",
    "            embed_dim=embed_dim,\n",
    "            vocabulary_size=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, tokenized_sequence):\n",
    "        \"\"\"\n",
    "        :param tokenized_sequence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        padding_mask = tokenized_sequence.eq(self.pad_token_id)\n",
    "\n",
    "        batch_size, seq_length = tokenized_sequence.size()\n",
    "\n",
    "        embedded_sequence = self.token_embedding(tokenized_sequence)\n",
    "\n",
    "        position_ids = (\n",
    "            torch.arange(seq_length, device=embedded_sequence.device)\n",
    "            .unsqueeze(0)\n",
    "            .expand(batch_size, seq_length)\n",
    "        )\n",
    "        positional_embeddings = self.positional_embedding(position_ids)\n",
    "\n",
    "        sequence = embedded_sequence + positional_embeddings\n",
    "\n",
    "        sequence = self.decoder_blocks(sequence, padding_mask)\n",
    "\n",
    "        logits = self.decoder(sequence)\n",
    "\n",
    "        return logits"
   ],
   "id": "f9e3b00b1fa46b44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialization",
   "id": "a7400b765d05aa11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer_model_path = \"../../saved_models/tokenizers/rotten_tomatoes_bpe_style\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_path)\n",
    "\n",
    "EMBED_DIM = 768\n",
    "NUM_HEADS = 8\n",
    "NUM_BLOCKS = 8\n",
    "MAX_SEQ_LENGTH = 256\n",
    "VOCAB_SIZE = len(tokenizer.get_vocab())\n",
    "\n",
    "model = GPTModel(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_blocks=NUM_BLOCKS,\n",
    "    max_seq_len=MAX_SEQ_LENGTH,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    tokenizer=tokenizer\n",
    ").to(device)"
   ],
   "id": "c771703b89e8d2b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data loading",
   "id": "8c8002fa352c9779"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tok = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH + 1\n",
    "    )\n",
    "    tok[\"input_ids\"] = [ids[:-1] for ids in tok[\"input_ids\"]]\n",
    "    tok[\"attention_mask\"] = [mask[:-1] for mask in tok[\"attention_mask\"]]\n",
    "    return tok\n",
    "\n",
    "\n",
    "tokenized = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    tokenized[split].set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\"]\n",
    "    )\n",
    "\n",
    "\n",
    "class CausalDataCollator:\n",
    "    def __init__(self, tokenizer, **kwargs):\n",
    "        self.base = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer, mlm=False, **kwargs\n",
    "        )\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        batch = self.base(examples)\n",
    "        inputs = batch[\"input_ids\"]\n",
    "        masks = batch[\"attention_mask\"]\n",
    "        batch[\"input_ids\"] = inputs[:, :-1]\n",
    "        batch[\"attention_mask\"] = masks[:, :-1]\n",
    "        labels = inputs[:, 1:].clone()\n",
    "        labels = labels.masked_fill(labels == tokenizer.pad_token_id, -100)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = CausalDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.0\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_dataloader = DataLoader(tokenized[\"train\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=data_collator)\n",
    "valid_dataloader = DataLoader(tokenized[\"validation\"], batch_size=BATCH_SIZE, shuffle=False, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(tokenized[\"test\"], batch_size=BATCH_SIZE, shuffle=False, collate_fn=data_collator)"
   ],
   "id": "668b8cfe793537be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training loop",
   "id": "d747ad7bf7cc9b83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "scaler = GradScaler()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "num_warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    "    num_cycles=0.5\n",
    ")\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "test_losses = []\n",
    "learning_rates = []\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # print(f\"E {(epoch + 1)}/{NUM_EPOCHS} - {((epoch + 1) / NUM_EPOCHS) * 100:.3f}%\")\n",
    "    logging.info(f\"E {(epoch + 1)}/{NUM_EPOCHS} - {((epoch + 1) / NUM_EPOCHS) * 100:.3f}%\")\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    for index, batch in enumerate(train_dataloader):\n",
    "        last_lr = scheduler.get_last_lr()[0]\n",
    "        learning_rates.append(last_lr)\n",
    "        if (index + 1) % 50 == 0:\n",
    "            # print(f\"\\tCurrent LR: {last_lr:10f}\")\n",
    "            logging.info(f\"\\tCurrent LR: {last_lr:10f}\")\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        # attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with autocast(\"cuda\"):\n",
    "            logits = model(input_ids)\n",
    "\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, len(tokenizer.get_vocab())),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        # clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if index == 0:\n",
    "            # probabilities = torch.softmax(output, dim=-1)\n",
    "            model_letters = torch.argmax(logits, dim=-1)\n",
    "            first_seq = model_letters[0]\n",
    "            decoded_expected_output = tokenizer.decode(\n",
    "                [token for token in labels[0].tolist() if token != -100],\n",
    "                skip_special_tokens=False\n",
    "            )\n",
    "            decoded_text = tokenizer.decode(first_seq.tolist(), skip_special_tokens=False)\n",
    "            print(f\"\\tTrain expected: {decoded_expected_output}\")\n",
    "            print(f\"\\tTrain output: {decoded_text}\")\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # print(f\"\\tTrain Loss: {train_loss:.5f}\")\n",
    "    logging.info(f\"\\tTrain Loss: {train_loss:.5f}\")\n",
    "\n",
    "    valid_loss = 0\n",
    "\n",
    "    for index, batch in enumerate(valid_dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        # attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with autocast(\"cuda\"):\n",
    "            logits = model(input_ids)\n",
    "\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, len(tokenizer.get_vocab())),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "        if index == 0:\n",
    "            # probabilities = torch.softmax(output, dim=-1)\n",
    "            model_letters = torch.argmax(logits, dim=-1)\n",
    "            first_seq = model_letters[0]\n",
    "            decoded_expected_output = tokenizer.decode(\n",
    "                [token for token in labels[0].tolist() if token != -100],\n",
    "                skip_special_tokens=False\n",
    "            )\n",
    "            decoded_text = tokenizer.decode(first_seq.tolist(), skip_special_tokens=False)\n",
    "            print(f\"\\tValid expected: {decoded_expected_output}\")\n",
    "            print(f\"\\tValid output: {decoded_text}\")\n",
    "\n",
    "    valid_loss /= len(valid_dataloader)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    # print(f\"\\tValid Loss: {valid_loss:.5f}\")\n",
    "    logging.info(f\"\\tValid Loss: {valid_loss:.5f}\")\n",
    "\n",
    "test_loss = 0\n",
    "\n",
    "for index, batch in enumerate(test_dataloader):\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    # attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "\n",
    "    with autocast(\"cuda\"):\n",
    "        logits = model(input_ids)\n",
    "\n",
    "        loss = loss_fn(\n",
    "            logits.view(-1, len(tokenizer.get_vocab())),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "\n",
    "    test_loss += loss.item()\n",
    "    test_losses.append(loss.item())\n",
    "\n",
    "    # probabilities = torch.softmax(output, dim=-1)\n",
    "    model_letters = torch.argmax(logits, dim=-1)\n",
    "    first_seq = model_letters[0]\n",
    "    # decoded_expected_output = tokenizer.decode(\n",
    "    #     [token for token in labels[0].tolist() if token != -100],\n",
    "    #     skip_special_tokens=False\n",
    "    # )\n",
    "    decoded_text = tokenizer.decode(first_seq.tolist(), skip_special_tokens=False)\n",
    "    # print(f\"\\t\\tTest expected: {decoded_expected_output}\")\n",
    "    print(f\"\\tTest output: {decoded_text}\")\n",
    "\n",
    "test_loss /= len(valid_dataloader)\n",
    "# print(f\"\\tTest Loss: {test_loss:.5f}\")\n",
    "logging.info(f\"\\tTest Loss: {test_loss:.5f}\")"
   ],
   "id": "cf85d042c558871a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes[0].plot(train_losses, label=\"Train loss\")\n",
    "axes[0].plot(valid_losses, label=\"Validation loss\")\n",
    "axes[0].set_title(\"Loss during training\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(test_losses, label=\"Test loss\")\n",
    "axes[1].set_title(\"Post training loss\")\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(learning_rates, label=\"LR values\")\n",
    "axes[2].set_title(\"Learning rate values over time\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "id": "5f85e3c1ea1e288f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
